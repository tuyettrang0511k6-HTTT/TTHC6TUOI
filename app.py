import streamlit as st
import os
import uuid
import chromadb
import google.generativeai as genai
from chromadb.utils import embedding_functions

# ================== C·∫§U H√åNH H·ªÜ TH·ªêNG ==================
CHROMA_DB_PATH = "./chroma_db"
COLLECTION_NAME = "dichvucong_rag"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3" # Model 1024 chi·ªÅu chuy√™n cho ti·∫øng Vi·ªát
GEMINI_MODEL_NAME = "gemini-1.5-flash" # B·∫°n c√≥ th·ªÉ ƒë·ªïi th√†nh gemini-2.0-flash n·∫øu API h·ªó tr·ª£

# ================== C·∫§U H√åNH API GEMINI ==================
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== H√ÄM KH·ªûI T·∫†O DATABASE ==================
@st.cache_resource
def get_vector_db():
    # Kh·ªüi t·∫°o h√†m embedding (Th·ªëng nh·∫•t d√πng BGE-M3)
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL_NAME
    )
    
    # Kh·ªüi t·∫°o Chroma Client
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # L·∫•y ho·∫∑c t·∫°o Collection
    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )
    return collection

collection = get_vector_db()

# ================== GIAO DI·ªÜN CH√çNH (UI) ==================
st.set_page_config(
    page_title="Chatbot Th·ªß t·ª•c Tr·∫ª em d∆∞·ªõi 6 tu·ªïi",
    page_icon="üë∂",
    layout="centered"
)

# Th√™m hi·ªáu ·ª©ng CSS v√† hoa r∆°i
st.markdown("""
<style>
    .stApp { background: #fff0f5; font-family: "Segoe UI", sans-serif; }
    h1, h2, h3 { color: #b91c5c; font-weight: 700; }
    div[data-testid="stChatMessageUser"] { background-color: #ffe4ec; border-radius: 14px; }
    div[data-testid="stChatMessageAssistant"] { background-color: #ffffff; border-radius: 14px; border: 1px solid #f3c6d3; }
    @keyframes fall {
        0% { transform: translateY(-50px) rotate(0deg); opacity: 0; }
        10% { opacity: 1; }
        100% { transform: translateY(110vh) rotate(360deg); opacity: 0; }
    }
    .flower { position: fixed; top: -40px; font-size: 22px; animation: fall linear infinite; z-index: 0; pointer-events: none; }
</style>
<div class="flower" style="left:10%; animation-duration:7s;">üå∏</div>
<div class="flower" style="left:30%; animation-duration:10s;">‚ú®</div>
<div class="flower" style="left:50%; animation-duration:6s;">üå∑</div>
<div class="flower" style="left:70%; animation-duration:9s;">üå∏</div>
<div class="flower" style="left:90%; animation-duration:8s;">‚ú®</div>
""", unsafe_allow_html=True)

st.title("ü§ñ Chatbot Th·ªß t·ª•c H√†nh ch√≠nh Tr·∫ª em")
st.info("H·ªó tr·ª£: Khai sinh, Th∆∞·ªùng tr√∫, Th·∫ª BHYT cho tr·∫ª em d∆∞·ªõi 6 tu·ªïi.")

# ================== X·ª¨ L√ù SIDEBAR ==================
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    top_k = st.slider("S·ªë l∆∞·ª£ng t√†i li·ªáu tham chi·∫øu (Top-k)", 1, 10, 3)
    
    st.divider()
    if st.button("üì• N·∫°p d·ªØ li·ªáu m·∫´u v√†o DB"):
        texts = [
            "Th·ªß t·ª•c ƒëƒÉng k√Ω khai sinh cho tr·∫ª em d∆∞·ªõi 6 tu·ªïi ƒë∆∞·ª£c th·ª±c hi·ªán t·∫°i UBND c·∫•p x√£ n∆°i c∆∞ tr√∫ c·ªßa cha ho·∫∑c m·∫π.",
            "H·ªì s∆° ƒëƒÉng k√Ω khai sinh g·ªìm: Gi·∫•y ch·ª©ng sinh, Gi·∫•y t·ªù t√πy th√¢n c·ªßa cha/m·∫π, Gi·∫•y ch·ª©ng nh·∫≠n k·∫øt h√¥n (n·∫øu c√≥).",
            "Tr·∫ª em d∆∞·ªõi 6 tu·ªïi ƒë∆∞·ª£c ng√¢n s√°ch nh√† n∆∞·ªõc ƒë√≥ng b·∫£o hi·ªÉm y t·∫ø v√† c·∫•p th·∫ª BHYT mi·ªÖn ph√≠.",
            "Th·ªß t·ª•c li√™n th√¥ng: Hi·ªán nay ng∆∞·ªùi d√¢n c√≥ th·ªÉ ƒëƒÉng k√Ω ƒë·ªìng th·ªùi Khai sinh, Th∆∞·ªùng tr√∫ v√† c·∫•p th·∫ª BHYT tr√™n C·ªïng d·ªãch v·ª• c√¥ng."
        ]
        metadatas = [
            {"hierarchy": "Khai sinh", "url": "https://dichvucong.gov.vn"},
            {"hierarchy": "H·ªì s∆°", "url": "https://dichvucong.gov.vn"},
            {"hierarchy": "BHYT", "url": "https://baohiemxahoi.gov.vn"},
            {"hierarchy": "Li√™n th√¥ng", "url": "https://dichvucong.gov.vn"}
        ]
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=[str(uuid.uuid4()) for _ in texts]
        )
        st.success("‚úÖ ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu v√†o Vector DB!")

    st.divider()
    st.write(f"üì¶ **DB:** {COLLECTION_NAME}")
    st.write(f"üß© **S·ªë chunk hi·ªán t·∫°i:** {collection.count()}")
    if st.button("üóëÔ∏è X√≥a s·∫°ch d·ªØ li·ªáu DB"):
        ids = collection.get()['ids']
        if ids:
            collection.delete(ids=ids)
            st.warning("ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu.")
            st.rerun()

# ================== H√ÄM TRUY V·∫§N RAG ==================
def query_rag(query_text):
    # 1. Retrieval
    results = collection.query(
        query_texts=[query_text],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    # 2. X√¢y d·ª±ng Context
    context_list = []
    if results["documents"][0]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            context_list.append(f"[{meta.get('hierarchy', 'N/A')}] {doc} (Ngu·ªìn: {meta.get('url', 'Internet')})")
    
    context = "\n\n".join(context_list)

    # 3. Prompt Engineering
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng Vi·ªát Nam chuy√™n v·ªÅ tr·∫ª em d∆∞·ªõi 6 tu·ªïi.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ CONTEXT ƒë·ªÉ tr·∫£ l·ªùi. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i ƒë√∫ng c√¢u: "Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

CONTEXT:
{context}

C√ÇU H·ªéI: {query_text}

Y√äU C·∫¶U:
- Tr√¨nh b√†y r√µ r√†ng, ƒë√°nh s·ªë th·ª© t·ª± n·∫øu c√≥ nhi·ªÅu b∆∞·ªõc.
- Tr√≠ch d·∫´n ngu·ªìn (URL) t·ª´ context ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi.
- Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát.
"""
    
    model = genai.GenerativeModel(GEMINI_MODEL_NAME)
    return model.generate_content(prompt, stream=True)

# ================== LOGIC CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Nh·∫≠n c√¢u h·ªèi
if prompt := st.chat_input("H·ªèi v·ªÅ th·ªß t·ª•c l√†m gi·∫•y khai sinh..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            response_stream = query_rag(prompt)
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "‚ñå")
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"L·ªói h·ªá th·ªëng: {str(e)}")
            full_response = "ƒê√£ c√≥ l·ªói x·∫£y ra khi k·∫øt n·ªëi v·ªõi AI."

    st.session_state.messages.append({"role": "assistant", "content": full_response})
