__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================== 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N & API ==================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DB_PATH = os.path.join(BASE_DIR, "chroma_db")

# Ki·ªÉm tra API KEY
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== 2. KH·ªûI T·∫†O EMBEDDING & COLLECTION ==================
# L∆∞u √Ω: Ph·∫£i d√πng ƒê√öNG model m√† b·∫°n ƒë√£ d√πng ·ªü m√°y Local (Colab)
# Theo ·∫£nh b·∫°n g·ª≠i l√† BAAI/bge-m3
@st.cache_resource
def load_collection():
    # 1. √âp s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(BASE_DIR, "chroma_db")
    
    chroma_client = chromadb.PersistentClient(path=db_path)

    # 2. Ph·∫£i d√πng ƒê√öNG model embedding ƒë√£ d√πng l√∫c t·∫°o database
    # Trong ·∫£nh b·∫°n g·ª≠i l√† BAAI/bge-m3, h√£y d√πng n√≥
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="BAAI/bge-m3"
    )

    # 3. S·ª≠ d·ª•ng get_collection (kh√¥ng d√πng get_or_create) ƒë·ªÉ ki·ªÉm tra
    # Ph·∫£i kh·ªõp t√™n "dichvucong_rag"
    collection = chroma_client.get_collection(
        name="dichvucong_rag", 
        embedding_function=embedding_func
    )

    return collection
# ================== 3. H√ÄM X·ª¨ L√ù TRUY V·∫§N (RAG) ==================
def query_rag(query: str, top_k: int):
    if not collection:
        return "Database ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng."

    # Truy v·∫•n d·ªØ li·ªáu
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ ph√π h·ª£p
    if not results["documents"] or len(results["documents"][0]) == 0:
        return "Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        hierarchy = meta.get('hierarchy', 'Th√¥ng tin')
        url = meta.get('url', 'Kh√¥ng c√≥ ngu·ªìn')
        context_parts.append(f"[{hierarchy}]\n{doc}\n(Ngu·ªìn: {url})")

    context = "\n\n".join(context_parts)

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.
Ch·ªâ s·ª≠ d·ª•ng CONTEXT sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi. N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i:
"Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

Context:
{context}

C√¢u h·ªèi: {query}
"""
    
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text

# ================== 4. GIAO DI·ªÜN STREAMLIT ==================
st.set_page_config(page_title="Chatbot TTHC Tr·∫ª Em", page_icon="ü§ñ")

# Hi·ªáu ·ª©ng hoa r∆°i (Gi·ªØ nguy√™n CSS c·ªßa b·∫°n)
st.markdown("""
<style>
.stApp { background: #fff0f5; }
.flower { position: fixed; top: -40px; font-size: 22px; animation: fall 8s linear infinite; z-index: 0; }
@keyframes fall { to { transform: translateY(110vh) rotate(360deg); } }
</style>
<div class="flower" style="left:10%">üå∏</div><div class="flower" style="left:30%">üå∑</div>
<div class="flower" style="left:50%">üåº</div><div class="flower" style="left:70%">üå∫</div>
""", unsafe_allow_html=True)

st.title("ü§ñ T∆∞ v·∫•n TTHC Tr·∫ª em d∆∞·ªõi 6 tu·ªïi")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    top_k = st.slider("S·ªë l∆∞·ª£ng chunk l·∫•y v·ªÅ", 1, 10, 3)
    st.divider()
    st.subheader("‚ÑπÔ∏è Th√¥ng tin h·ªá th·ªëng")
    if collection:
        st.success(f"‚úÖ ƒê√£ k·∫øt n·ªëi Database")
        st.write(f"üß© S·ªë chunk: {collection.count()}")
    else:
        st.error("‚ùå Ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu")

# L·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# X·ª≠ l√Ω nh·∫≠p li·ªáu
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang t√¨m ki·∫øm d·ªØ li·ªáu..."):
            answer = query_rag(prompt, top_k)
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
