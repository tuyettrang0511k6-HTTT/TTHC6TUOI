import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================== C·∫§U H√åNH ==================
CHROMA_DB_PATH = "chroma_db"
COLLECTION_NAME = "tthc_collection"

# üîë L·∫§Y ƒê∆Ø·ªúNG D·∫™N TUY·ªÜT ƒê·ªêI THEO FILE app.py (KH√îNG BAO GI·ªú L·ªñI)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
JSON_FILE = os.path.join(BASE_DIR, "data", "all_procedures_normalized.json")

EMBEDDING_MODEL = "BAAI/bge-m3"
GEMINI_MODEL = "gemini-1.5-flash"

# ================== KI·ªÇM TRA API KEY ==================
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== LOAD CHROMA COLLECTION ==================
@st.cache_resource
def load_collection():
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )
    return collection

# ================== LOAD JSON ‚Üí ADD V√ÄO CHROMA (CH·∫†Y 1 L·∫¶N) ==================
def load_json_to_chroma(collection, json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents, metadatas, ids = [], [], []

    for i, item in enumerate(data):
        documents.append(item["content"])
        metadatas.append({
            "hierarchy": item.get("hierarchy", ""),
            "url": item.get("url", ""),
            "source_file": item.get("source_file", "")
        })
        ids.append(f"doc_{i}")

    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )

# ================== KH·ªûI T·∫†O DB ==================
collection = load_collection()

# DEBUG an to√†n (c√≥ th·ªÉ xo√° sau)
st.sidebar.write("üìÑ JSON exists:", os.path.exists(JSON_FILE))

if collection.count() == 0:
    st.warning("üì• ƒêang n·∫°p d·ªØ li·ªáu v√†o Vector DB...")
    load_json_to_chroma(collection, JSON_FILE)
    st.success(f"‚úÖ ƒê√£ n·∫°p {collection.count()} chunks")

# ================== H√ÄM RAG QUERY ==================
def query_rag(query: str, top_k: int):
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    if not results["documents"][0]:
        return None

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        context_parts.append(
            f"[BLOCK: {meta['hierarchy']}]\n"
            f"{doc}\n"
            f"NGU·ªíN: {meta['url']}"
        )

    return "\n\n".join(context_parts)

# ================== G·ªåI GEMINI ==================
def call_gemini(context, question):
    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong CONTEXT.
Kh√¥ng s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i.
Kh√¥ng nh·∫Øc l·∫°i c√¢u h·ªèi.

N·∫øu CONTEXT kh√¥ng li√™n quan, ch·ªâ tr·∫£ l·ªùi ƒë√∫ng c√¢u:
"Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

CONTEXT:
{context}

C√¢u h·ªèi:
{question}

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, c√≥ ƒë√°nh s·ªë n·∫øu c·∫ßn.
Gi·ªØ nguy√™n d√≤ng NGU·ªíN.
"""

    model = genai.GenerativeModel(GEMINI_MODEL)
    response = model.generate_content(prompt)
    return response.text

# ================== GIAO DI·ªÜN STREAMLIT ==================
st.set_page_config(
    page_title="Chatbot TTHC tr·∫ª em d∆∞·ªõi 6 tu·ªïi",
    page_icon="ü§ñ",
    layout="centered"
)

st.title("ü§ñ Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh")
st.markdown(
    "H·ªó tr·ª£ **ƒëƒÉng k√Ω khai sinh ‚Äì th∆∞·ªùng tr√∫ ‚Äì BHYT** "
    "cho **tr·∫ª em d∆∞·ªõi 6 tu·ªïi** t·ª´ d·ªØ li·ªáu ch√≠nh th·ªëng."
)

with st.sidebar:
    st.markdown("## ‚öôÔ∏è C·∫•u h√¨nh")
    top_k = st.slider("Top-k retrieval", 1, 10, 3)
    st.divider()
    st.write(f"üì¶ Vector DB: {COLLECTION_NAME}")
    st.write(f"üß© S·ªë chunk: {collection.count()}")
    st.write(f"üìê Embedding: {EMBEDDING_MODEL}")
    st.write(f"ü§ñ LLM: {GEMINI_MODEL}")

# ================== SESSION CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        context = query_rag(prompt, top_k)

        if context is None:
            answer = "Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."
        else:
            answer = call_gemini(context, prompt)

        st.markdown(answer)

    st.session_state.messages.append(
        {"role": "assistant", "content": answer}
    )
