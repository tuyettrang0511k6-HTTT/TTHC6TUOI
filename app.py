import streamlit as st
import os
import uuid
import chromadb
import google.generativeai as genai
from chromadb.utils import embedding_functions

# ================== C·∫§U H√åNH H·ªÜ TH·ªêNG ==================
CHROMA_DB_PATH = "./chroma_db"
COLLECTION_NAME = "dichvucong_rag"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3" 
GEMINI_MODEL_NAME = "gemini-1.5-flash" # Model ph·ªï bi·∫øn v√† ·ªïn ƒë·ªãnh nh·∫•t

# ================== C·∫§U H√åNH API GEMINI ==================
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== H√ÄM KH·ªûI T·∫†O DATABASE ==================
@st.cache_resource
def get_vector_db():
    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL_NAME
    )
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )
    return collection

collection = get_vector_db()

# ================== GIAO DI·ªÜN CH√çNH (UI) ==================
st.set_page_config(page_title="Chatbot Th·ªß t·ª•c Tr·∫ª em", page_icon="üë∂")

st.markdown("""
<style>
    .stApp { background: #fff0f5; }
    h1 { color: #b91c5c; }
    div[data-testid="stChatMessageAssistant"] { background-color: #ffffff; border: 1px solid #f3c6d3; }
</style>
""", unsafe_allow_html=True)

st.title("ü§ñ Chatbot Th·ªß t·ª•c H√†nh ch√≠nh Tr·∫ª em")

# ================== X·ª¨ L√ù SIDEBAR ==================
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    top_k = st.slider("S·ªë l∆∞·ª£ng t√†i li·ªáu (Top-k)", 1, 10, 3)
    
    if st.button("üì• N·∫°p d·ªØ li·ªáu m·∫´u"):
        texts = [
            "Th·ªß t·ª•c ƒëƒÉng k√Ω khai sinh cho tr·∫ª em d∆∞·ªõi 6 tu·ªïi th·ª±c hi·ªán t·∫°i UBND c·∫•p x√£.",
            "H·ªì s∆° g·ªìm: Gi·∫•y ch·ª©ng sinh, CCCD c·ªßa cha m·∫π, Gi·∫•y k·∫øt h√¥n.",
            "Tr·∫ª d∆∞·ªõi 6 tu·ªïi ƒë∆∞·ª£c c·∫•p th·∫ª BHYT mi·ªÖn ph√≠."
        ]
        metadatas = [
            {"hierarchy": "Khai sinh", "url": "https://dichvucong.gov.vn"},
            {"hierarchy": "H·ªì s∆°", "url": "https://dichvucong.gov.vn"},
            {"hierarchy": "BHYT", "url": "https://baohiemxahoi.gov.vn"}
        ]
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=[str(uuid.uuid4()) for _ in texts]
        )
        st.success("‚úÖ ƒê√£ n·∫°p d·ªØ li·ªáu!")

    if st.button("üóëÔ∏è X√≥a s·∫°ch d·ªØ li·ªáu"):
        ids = collection.get()['ids']
        if ids: collection.delete(ids=ids)
        st.rerun()

# ================== H√ÄM TRUY V·∫§N RAG ==================
def query_rag(query_text):
    results = collection.query(
        query_texts=[query_text],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    context_list = []
    if results["documents"][0]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            context_list.append(f"[{meta.get('hierarchy', 'N/A')}] {doc} (Ngu·ªìn: {meta.get('url', 'Internet')})")
    
    context = "\n\n".join(context_list)

    # L∆ØU √ù: Th√™m ti·ªÅn t·ªë 'models/' ƒë·ªÉ tr√°nh l·ªói 404
    model = genai.GenerativeModel(model_name=f"models/{GEMINI_MODEL_NAME}")
    
    prompt = f"S·ª≠ d·ª•ng context sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Context: {context}\n\nC√¢u h·ªèi: {query_text}"
    return model.generate_content(prompt, stream=True)

# ================== LOGIC CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            response_stream = query_rag(prompt)
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "‚ñå")
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"L·ªói: {str(e)}")
            full_response = "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi."

    st.session_state.messages.append({"role": "assistant", "content": full_response})
