
import streamlit as st
import os
import json

import chromadb
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer

# ====== Cáº¤U HÃŒNH ======
CHROMA_DB_PATH = "./chroma_db"

# ====== KIá»‚M TRA API KEY ======
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("âŒ ChÆ°a cáº¥u hÃ¬nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

# ====== KHá»I Táº O GEMINI CLIENT ======
import google.generativeai as genai

# ====== KIá»‚M TRA API KEY ======
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("âŒ ChÆ°a cáº¥u hÃ¬nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

# ====== Cáº¤U HÃŒNH & KHá»I Táº O GEMINI ======
genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
model = genai.GenerativeModel('gemini-1.5-flash')

# ====== CÃCH Gá»ŒI KHI Äáº¶T CÃ‚U Há»I ======
# response = model.generate_content("CÃ¢u há»i cá»§a báº¡n")
# st.write(response.text)




# ================== Cáº¤U HÃŒNH ==================
JSON_FILE = "/content/drive/RAG/all_procedures_normalized.json"  # ÄÆ°á»ng dáº«n file JSON (sau chunk rule-based)
CHROMA_DB_PATH = "chroma_db"  # ThÆ° má»¥c lÆ°u vector DB
COLLECTION_NAME = "dichvucong_rag"
GEMINI_MODEL = "gemini-2.5-flash"  # Hoáº·c "gemini-1.5-pro"

@st.cache_resource
def get_embedding_function():
    EMBEDDING_MODEL = "BAAI/bge-m3"  # Model embedding tiáº¿ng Viá»‡t
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="BAAI/bge-m3")
    return embedding_function

@st.cache_resource
def load_collection():
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="BAAI/bge-m3"
    )

    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )

    return collection

# --- Load collection 1 láº§n ---
collection = load_collection()


def query_rag(query: str, chat_history: list, top_k: int):
    # Retrieval vá»›i top_k Ä‘á»™ng
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        context_parts.append(f"[{meta['hierarchy']}]\\n{doc}\\n(Nguá»“n: {meta['url']})")

    context = "\\n\\n".join(context_parts)

    prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n thá»§ tá»¥c hÃ nh chÃ­nh cÃ´ng cá»§a Viá»‡t Nam.
Báº¡n chá»‰ tráº£ lá»i cÃ¢u há»i.
KHÃ”NG Ä‘Æ°á»£c viáº¿t láº¡i, diá»…n Ä‘áº¡t láº¡i hay sá»­a Ä‘á»•i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.
KHÃ”NG nháº¯c láº¡i cÃ¢u há»i.
PHáº M VI ÃP Dá»¤NG:
- Æ¯u tiÃªn tÆ° váº¥n cÃ¡c thá»§ tá»¥c hÃ nh chÃ­nh liÃªn quan Ä‘áº¿n tráº» em dÆ°á»›i 6 tuá»•i.
- Náº¿u CONTEXT khÃ´ng Ä‘á» cáº­p rÃµ Ä‘á»™ tuá»•i nhÆ°ng ná»™i dung thuá»™c thá»§ tá»¥c thÆ°á»ng Ã¡p dá»¥ng cho tráº» em,
  báº¡n Ä‘Æ°á»£c phÃ©p tráº£ lá»i dá»±a trÃªn thÃ´ng tin hiá»‡n cÃ³ vÃ  nÃªu rÃµ pháº¡m vi Ã¡p dá»¥ng náº¿u Ä‘Æ°á»£c Ä‘á» cáº­p.

NGUYÃŠN Táº®C TRáº¢ Lá»œI:
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong CONTEXT bÃªn dÆ°á»›i.
- KhÃ´ng sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i.
- KhÃ´ng tá»± bá»• sung thÃ´ng tin khÃ´ng cÃ³ trong CONTEXT.
- KhÃ´ng tá»± thay Ä‘á»•i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

CÃCH TRáº¢ Lá»œI:
- Chá»‰ tráº£ lá»i cÃ¡c ná»™i dung LIÃŠN QUAN TRá»°C TIáº¾P Ä‘áº¿n cÃ¢u há»i.
- CÃ³ thá»ƒ tá»•ng há»£p nhiá»u Ä‘oáº¡n trong CONTEXT náº¿u chÃºng cÃ¹ng mÃ´ táº£ má»™t thá»§ tá»¥c.
- TrÃ¬nh bÃ y ngáº¯n gá»n, rÃµ rÃ ng, Ä‘Ãºng trá»ng tÃ¢m.

TRÆ¯á»œNG Há»¢P KHÃ”NG TRáº¢ Lá»œI:
Chá»‰ tráº£ lá»i Ä‘Ãºng cÃ¢u sau náº¿u:
- CONTEXT hoÃ n toÃ n khÃ´ng chá»©a thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i.

CÃ¢u tráº£ lá»i trong trÆ°á»ng há»£p nÃ y PHáº¢I CHÃNH XÃC:
"Xin lá»—i! CÃ¢u há»i cá»§a báº¡n khÃ´ng náº±m trong pháº¡m vi há»— trá»£ cá»§a tÃ´i."

YÃŠU Cáº¦U Äá»ŠNH Dáº NG:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.
- Náº¿u cÃ³ nhiá»u Ã½, trÃ¬nh bÃ y báº±ng gáº¡ch Ä‘áº§u dÃ²ng hoáº·c Ä‘Ã¡nh sá»‘.
- Giá»¯ nguyÃªn trÃ­ch dáº«n nguá»“n náº¿u cÃ³ trong CONTEXT.

    Context:
    {context}

    CÃ¢u há»i: {query}

    Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, cÃ³ Ä‘Ã¡nh sá»‘ náº¿u lÃ  danh sÃ¡ch, vÃ  trÃ­ch dáº«n nguá»“n rÃµ rÃ ng (tÃªn block, URL):
    """

    model = genai.GenerativeModel(GEMINI_MODEL)
    response = model.generate_content(prompt, stream=True)

    return response

# ================== GIAO DIá»†N CHÃNH ==================
st.set_page_config(
    page_title="Chatbot tÆ° váº¥n thá»§ tá»¥c hÃ nh chÃ­nh tráº» em dÆ°á»›i 6 tuá»•i",
    page_icon="ğŸ¤–",
    layout="centered"
)

# ================== TIÃŠU Äá»€ ==================
st.title("ğŸ¤– Chatbot tÆ° váº¥n thá»§ tá»¥c hÃ nh chÃ­nh tráº» em dÆ°á»›i 6 tuá»•i")
st.markdown(
    "Há»— trá»£ tÆ° váº¥n **Ä‘Äƒng kÃ½ khai sinh â€“ Ä‘Äƒng kÃ½ thÆ°á»ng trÃº â€“ cáº¥p tháº» BHYT** "
    "cho **tráº» em dÆ°á»›i 6 tuá»•i** dá»±a trÃªn dá»¯ liá»‡u chÃ­nh thá»‘ng."
)
# Sidebar vá»›i top-k slider vÃ  thÃ´ng tin
with st.sidebar:
    top_k = st.slider("Top-k retrieval (sá»‘ chunks láº¥y vá»)", min_value=1, max_value=10, value=3, step=1)
st.markdown(
"""
<style>
/* Ná»n toÃ n app: há»“ng nháº¡t */
.stApp {
    background: #fff0f5;
    font-family: "Segoe UI", sans-serif;
}

/* Sidebar */
section[data-testid="stSidebar"] {
    background-color: #ffffff;
    border-right: 1px solid #f3c6d3;
    padding: 16px;
}

/* TiÃªu Ä‘á» */
h1, h2, h3 {
    color: #b91c5c;
    font-weight: 700;
}

/* Bong bÃ³ng chat */
div[data-testid="stChatMessageUser"] {
    background-color: #ffe4ec;
    border-radius: 14px;
    padding: 12px;
    margin-bottom: 8px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.05);
}

div[data-testid="stChatMessageAssistant"] {
    background-color: #ffffff;
    border-radius: 14px;
    padding: 12px;
    margin-bottom: 8px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.05);
}

/* Hoa rÆ¡i */
@keyframes fall-random {
    0% {
        transform: translate(0, -50px) rotate(0deg);
        opacity: 0;
    }
    10% { opacity: 1; }
    100% {
        transform: translate(var(--x-move), 110vh) rotate(360deg);
        opacity: 0;
    }
}

.flower {
    position: fixed;
    top: -40px;
    font-size: 22px;
    animation: fall-random linear infinite;
    z-index: 0;
    pointer-events: none;
}
</style>

<div class="flower" style="left:5%;  --x-move:-80px; animation-duration:6s;">ğŸŒ¸</div>
<div class="flower" style="left: 20%; --x-move:-100px; animation-duration: 4s;">ğŸ§¨</div>
<div class="flower" style="left:15%; --x-move:120px; animation-duration:7s;">ğŸŒ·</div>
<div class="flower" style="left:30%; --x-move:-60px; animation-duration:7.5s;">ğŸ’</div>
<div class="flower" style="left:37%; --x-move:70px; animation-duration:8s;">âœ¨</div>
<div class="flower" style="left:25%; --x-move:-150px; animation-duration:8s;">ğŸŒ¼</div>
<div class="flower" style="left: 50%; --x-move:-100px; animation-duration: 4s;">ğŸ§¨</div>
<div class="flower" style="left:35%; --x-move:90px; animation-duration:6.5s;">ğŸŒº</div>
<div class="flower" style="left: 85%; --x-move:130px; animation-duration: 15s;">ğŸ€</div>
<div class="flower" style="left:45%; --x-move:-60px; animation-duration:7.5s;">ğŸ’</div>
<div class="flower" style="left:55%; --x-move:140px; animation-duration:9s;">ğŸŒ¸</div>
<div class="flower" style="left: 85%; --x-move:130px; animation-duration: 15s;">ğŸ€</div>
<div class="flower" style="left:65%; --x-move:-120px; animation-duration:6.8s;">ğŸŒ·</div>
<div class="flower" style="left: 81%; --x-move:-100px; animation-duration: 4s;">ğŸ§¨</div>
<div class="flower" style="left:75%; --x-move:70px; animation-duration:8.2s;">ğŸŒ¼</div>
<div class="flower" style="left:40%; --x-move:-100px; animation-duration:7.2s;">ğŸŒº</div>
<div class="flower" style="left:99%; --x-move:70px; animation-duration:8s;">âœ¨</div>
""",
unsafe_allow_html=True
)


# ================== SIDEBAR ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    st.markdown("## ğŸ“œ Lá»‹ch sá»­ trÃ² chuyá»‡n")

    if st.session_state.messages:
        for i, msg in enumerate(st.session_state.messages):
            if msg["role"] == "user":
                st.markdown(f"**ğŸ‘¤ NgÆ°á»i dÃ¹ng:** {msg['content']}")
            else:
                st.markdown(f"**ğŸ¤– Chatbot:** {msg['content'][:150]}...")
            st.divider()
    else:
        st.caption("ChÆ°a cÃ³ cuá»™c trÃ² chuyá»‡n nÃ o.")

    if collection:
        try:
           data = collection.get(include=["metadatas"])
           metadatas = data.get("metadatas", [])

           source_files = set()

           for meta in metadatas:
              if not meta:
                continue

              file_name = meta.get("source_file", "").strip()
              if file_name:
                source_files.add(file_name)

        except Exception as e:
            st.error(f"Lá»—i khi táº£i file dá»¯ liá»‡u: {e}")
    else:
        st.caption("ChÆ°a táº£i Ä‘Æ°á»£c dá»¯ liá»‡u vector.")


    st.divider()

    st.markdown("## â„¹ï¸ ThÃ´ng tin há»‡ thá»‘ng")
    st.write(f"ğŸ“¦ Vector DB: {COLLECTION_NAME}")
    st.write(f"ğŸ§© Sá»‘ chunk: {collection.count() if collection else 0}")
    st.write(f"ğŸ¤– LLM: {GEMINI_MODEL}")
    st.write("ğŸ“ Embedding: BAAI/bge-m3")
    st.caption("Dá»¯ liá»‡u Ä‘Æ°á»£c load tá»« file JSON.")

# ================== KHá»I Táº O Lá»ŠCH Sá»¬ CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

# ================== HIá»‚N THá»Š Lá»ŠCH Sá»¬ CHAT ==================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ================== INPUT Tá»ª USER ==================
prompt = st.chat_input(
    "Nháº­p cÃ¢u há»i cá»§a báº¡n. "
    "(VÃ­ dá»¥: Giáº¥y khai sinh cÃ³ cáº¥p báº£n Ä‘iá»‡n tá»­ khÃ´ng?)"
)

if prompt:
    # LÆ°u cÃ¢u há»i
    st.session_state.messages.append(
        {"role": "user", "content": prompt}
    )

    with st.chat_message("user"):
        st.markdown(prompt)

    # ================== Gá»ŒI BACKEND (GIá»® NGUYÃŠN) ==================
    with st.chat_message("assistant"):
        full_response = ""
        message_placeholder = st.empty()

        try:
            response = query_rag(prompt, st.session_state.messages, top_k)
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response)
            message_placeholder.markdown(full_response)
        except Exception as e:
            full_response = f"Lá»—i khi gá»i Gemini: {str(e)}"
            message_placeholder.error(full_response)


    # LÆ°u cÃ¢u tráº£ lá»i
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response}
    )
