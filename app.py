import streamlit as st
import json
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================== C·∫§U H√åNH ==================
JSON_FILE = "data/all_procedures_normalized.json"
COLLECTION_NAME = "dichvucong_rag"
GEMINI_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "BAAI/bge-m3"

# ================== KI·ªÇM TRA API KEY ==================
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== LOAD + CHUNK DATA ==================
def load_and_chunk_data():
    with open(JSON_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    docs, metas, ids = [], [], []

    for i, item in enumerate(data):
        content = item.get("content", "").strip()
        if not content:
            continue

        docs.append(content)
        metas.append({
            "hierarchy": item.get("hierarchy", "N/A"),
            "url": item.get("url", "N/A"),
            "source_file": item.get("source_file", "json")
        })
        ids.append(str(i))

    return docs, metas, ids

# ================== LOAD + INGEST VECTOR DB ==================
@st.cache_resource
def load_collection():
    client = chromadb.Client()

    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )

    if collection.count() == 0:
        docs, metas, ids = load_and_chunk_data()
        collection.add(
            documents=docs,
            metadatas=metas,
            ids=ids
        )

    return collection

collection = load_collection()

# ================== QUERY RAG ==================
def query_rag(query: str, top_k: int):
    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas"]
    )

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        context_parts.append(
            f"[{meta.get('hierarchy')}]\n{doc}\n(Ngu·ªìn: {meta.get('url')})"
        )

    context = "\n\n".join(context_parts)

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.

CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong CONTEXT.
KH√îNG d√πng ki·∫øn th·ª©c b√™n ngo√†i.

N·∫øu CONTEXT kh√¥ng li√™n quan, tr·∫£ l·ªùi ƒë√∫ng c√¢u:
"Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

CONTEXT:
{context}

C√¢u h·ªèi: {query}
"""

    model = genai.GenerativeModel(GEMINI_MODEL)
    response = model.generate_content(prompt, stream=True)
    return response

# ================== GIAO DI·ªÜN ==================
st.set_page_config(
    page_title="Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh tr·∫ª em d∆∞·ªõi 6 tu·ªïi",
    page_icon="ü§ñ"
)

st.title("ü§ñ Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh tr·∫ª em d∆∞·ªõi 6 tu·ªïi")

with st.sidebar:
    top_k = st.slider("Top-k retrieval", 1, 10, 3)
    st.write(f"üì¶ Vector DB: {COLLECTION_NAME}")
    st.write(f"üß© S·ªë chunk: {collection.count()}")
    st.write(f"üìê Embedding: {EMBEDDING_MODEL}")
    st.write(f"ü§ñ LLM: {GEMINI_MODEL}")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        full_response = ""
        placeholder = st.empty()

        try:
            response = query_rag(prompt, top_k)
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    placeholder.markdown(full_response)
        except Exception as e:
            full_response = f"L·ªói: {e}"
            placeholder.error(full_response)

    st.session_state.messages.append(
        {"role": "assistant", "content": full_response}
    )
