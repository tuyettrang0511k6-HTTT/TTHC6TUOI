__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import streamlit as st
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================== 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N & API ==================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_DB_PATH = os.path.join(BASE_DIR, "chroma_db")

# Ki·ªÉm tra API KEY
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets")
    st.stop()

genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ================== 2. KH·ªûI T·∫†O EMBEDDING & COLLECTION ==================
@st.cache_resource
def load_collection():
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="BAAI/bge-m3"
    )

    collection = chroma_client.get_collection(
        name="dichvucong_rag",
        embedding_function=embedding_func
    )

    return collection


# ‚úÖ FIX L·ªñI QUAN TR·ªåNG: PH·∫¢I G·ªåI H√ÄM
try:
    collection = load_collection()
except Exception as e:
    collection = None
    st.error(f"‚ùå L·ªói load database: {e}")

# ================== 3. H√ÄM X·ª¨ L√ù TRUY V·∫§N (RAG) ==================
def query_rag(query: str, top_k: int):
    if collection is None:
        return "Database ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng."

    results = collection.query(
        query_texts=[query],
        n_results=top_k,
        include=["documents", "metadatas", "distances"]
    )

    if not results["documents"] or len(results["documents"][0]) == 0:
        return "Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

    context_parts = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        hierarchy = meta.get("hierarchy", "Th√¥ng tin")
        url = meta.get("url", "Kh√¥ng c√≥ ngu·ªìn")
        context_parts.append(f"[{hierarchy}]\n{doc}\n(Ngu·ªìn: {url})")

    context = "\n\n".join(context_parts)

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.
Ch·ªâ s·ª≠ d·ª•ng CONTEXT sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi.
N·∫øu CONTEXT kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i:
"Xin l·ªói! C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng n·∫±m trong ph·∫°m vi h·ªó tr·ª£ c·ªßa t√¥i."

Context:
{context}

C√¢u h·ªèi: {query}
"""

    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"‚ùå L·ªói khi g·ªçi Gemini: {e}"

# ================== 4. GIAO DI·ªÜN STREAMLIT ==================
st.set_page_config(page_title="Chatbot TTHC Tr·∫ª Em", page_icon="ü§ñ")

st.markdown("""
<style>
.stApp { background: #fff0f5; }
.flower { position: fixed; top: -40px; font-size: 22px; animation: fall 8s linear infinite; z-index: 0; }
@keyframes fall { to { transform: translateY(110vh) rotate(360deg); } }
</style>
<div class="flower" style="left:10%">üå∏</div>
<div class="flower" style="left:30%">üå∑</div>
<div class="flower" style="left:50%">üåº</div>
<div class="flower" style="left:70%">üå∫</div>
""", unsafe_allow_html=True)

st.title("ü§ñ T∆∞ v·∫•n TTHC Tr·∫ª em d∆∞·ªõi 6 tu·ªïi")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    top_k = st.slider("S·ªë l∆∞·ª£ng chunk l·∫•y v·ªÅ", 1, 10, 3)
    st.divider()
    st.subheader("‚ÑπÔ∏è Th√¥ng tin h·ªá th·ªëng")

    if collection is not None:
        st.success("‚úÖ ƒê√£ k·∫øt n·ªëi Database")
        st.write(f"üß© S·ªë chunk: {collection.count()}")
    else:
        st.error("‚ùå Ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu")

# ================== CHAT ==================
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang t√¨m ki·∫øm d·ªØ li·ªáu..."):
            answer = query_rag(prompt, top_k)
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
